\Section{Results}
\label{sec:results}

\subsection{Results of the Hyperparameter Optimization}
\label{subsec:perfmlp}

The hyperparameter optimization is performed both with Bayesian optimization and with random search for $50$ trials to compare both algorithms. For each method the history plot, 
parallel coordinate plot and parameter importanace plot is presented. The history plot shows the loss value achieved for each trial, where the red line indicates what loss value yet achieved is best.
The history plot for Bayesian optimization can be seen in Fig. \ref{fig:1}, the one for random search in Fig. \ref{fig:2}. 

\Figure{H}{0.8}{fig:1}{History plot of Bayesian optimization}{ - The loss value of each trial}{content/Plots/history_plot_bay_final.png}
\Figure{H}{0.8}{fig:2}{History plot of random search}{ - The loss value of each trial}{content/Plots/history_plot_random_final.png}

For both algorithms it can be seen that within the first ten trials the loss value 
is improved rapidly, but soon the loss only improves minimally anymore, so it is not expected that a significantly better result would be achieved after more trials for both algorithms. 
In addition it can be seen that for the Bayesian optimization many trials achieve a loss value close to the best yet achieved loss. Only in seven out of $50$ trials the
sampling point led to a loss value that had a larger distance than $0.2$ to the best yet achieved loss. For the random sampler the loss values 
are equally distributed over a wider loss-range, which visualizes that the Bayesian optimization suggests the next sampling point educated in contrast to the random samper.
The best validation loss value for the Bayesian optimization,

\eqn{
    \mathcal{L} = 0.8878
}

was achieved in trial $18$. The lowest validation loss achieved through random search,

\eqn{
    \mathcal{L} = 0.8891
}

was reached in trial $27$. 

The parallel coordinate plot of the Bayesian optimization is shown in Fig. \ref{fig:3}, the one for the random search in Fig. \ref{fig:4}.

\Figure{H}{0.8}{fig:3}{Parallel coordinate plot of Bayesian optimization}{ - All tested hyperparameter combinations are visualized together with the achieved loss value.}{content/Plots/parallel_coordinate_plot_bayesian.png}
\Figure{H}{0.8}{fig:4}{Parallel coordinate plot of random search}{ - All tested hyperparameter combinations are visualized together with the achieved loss value.}{content/Plots/parallel_coordinate_plot_random.png}


The algorithm of the Bayesian is visualized:
It can be seen how intervals of hyperparameters, where the loss value is low, are chosen far more often then values, where the loss is higher. When looking at the learning rate for example
one can observe that in far more trials a value between $1 \cdot 10^{-5}$ and $1 \cdot 10^{-4}$ than between $1 \cdot 10^{-6}$ and $1 \cdot 10^{-5}$ is chosen. Learning rates in the second interval
lead to a high objective value, so this area is barely exploited. The same behaviour can be seen for the other hyperparameters as well.
For the random search by contrast such a behaviour can not be observed, hyperparameter combinations with a higher loss seem to be as often tested as combinations with a lower loss and the chosen
hyperparameter values are equally distributed in the given interval.

Finally the importance of different hyperparameters is shown in Fig. \ref{fig:5} for the Bayesian optimization and in Fig. \ref{fig:6} for the random search.

\Figure{H}{0.8}{fig:5}{Parameter importance plot of Bayesian optimization}{ - The importance of every optimized hyperparameter on the loss value.}{content/Plots/param_importances_plot_bayesian.png}
\Figure{H}{0.8}{fig:6}{Parameter importance plot of random search}{ - The importance of every optimized hyperparameter on the loss value.}{content/Plots/param_importance_random.png}

For both algorithms the learning rate is by far the most important hyperparameter with an importance of $70 \%$, followed by the number of layers, dropout probability and weight decay.
The order of importance of these three hyperparameters varies between the two algoritms, although both algorithms work with the same model. 
This illustrates a weakness of the determination of hyperparameter importance: The importance is evaluated by changing one hyperparameter of a given hyperparameter set at a time while observing how much the loss changes.
Depending on the values of the other hyperparameters, the loss will change slightly more or less when one parameter is varied; the result of this variation always depends on the given
hyperparameter set and this differs between the studies. In both studies the number of nodes and choice of activation function are the least important hyperparameters. \\

The best hyperparameters for the Bayesian optimization are shown in this Table:

\Table{H}{tab:5e}{Hyperparameters for Bayesian optimization}{}{c c}{
    \hline
    Hyperparameter & Result \\
    \hline
    Number of layers & $4$ \\
    Number of nodes & $1024$ \\
    Activation function & SELU \\
    Learning rate & $4.1511 \cdot 10^{-5}$\\
    Weight decay & $7.4375 \cdot 10^{-6}$\\
    Dropout probability & $0.2221$\\
    \hline
}

These are the best hyperparameters for the random search:

\Table{H}{tab:5e}{Hyperparameters for random search}{}{c c}{
    \hline
    Hyperparameter & Result \\
    \hline
    Number of layers & $4$ \\
    Number of nodes & $256$ \\
    Activation function & SELU \\
    Learning rate & $6.1851 \cdot 10^{-5}$ \\
    Weight decay & $7.4634  \cdot 10^{-6}$ \\
    Dropout probability & $0.0881$\\
    \hline
}

The hyperparameters selected for the two studies are mostly similar. The biggest difference is that the dropout probability is lower for the the random search, while the learning rate is slightly higher here. 
Since the loss value of the Bayesian optimization is slightly lower than the one of the random search, these hyperparameters are used to train the MLP.
When comparing Bayesian optimization and random search it can be said that it does not make a large difference which algorithm is used. The loss values only differed in the third decimal place
and were achieved only nine trials apart. Since the Bayesian optimization is a far more complex algorithm than the random search it can be said that this complexity does not show in the results,
so in this thesis a random search algorithm would be completely sufficient as well.

\subsection{Performance of the Neural Networks}
\label{subsec:perfgat}

The results of the Bayesian hyperparameter optimization are chosen as the hyperparameters for the MLP. A batch size of $512$ is used, the early stopping patience is $75$ epochs and the maximum
number of epochs is $400$. After each epoch it is checked if the weights lead to the highest validation loss that was yet achieved. If that is the case, these weights are saved and selected as the best model,
which will be used further to measure the performance. \\

The data are splitted into a training set, validation set and test set with a ratio of $0.6$, $0.2$ and $0.2$. Then a standardization according to the train data set is performed: For the training data,
the mean and standard deviation are caluclated for each variable. After that the data from all three datasets are standardized with these values,

\eqn{
    x_{i, stand.} = \frac{x_i - \langle x_i \rangle}{\sigma_i}
}

where the index $i$ indicates the variable. \\

As seen in chapter \ref{sec:HiggsDNA} there is a different number of events for each class and especially there are far more non-resonant background events than events of other classes, so it is likely
that the model favors this class. The goal is that the model only differentiates the classes based on the input variables and that an uneven distribution of number of events per class does not 
falsify the training process. In order to achieve that, different weights for each class and event are used.
First each event has an individual weight, which is normalized with the sum of weights for all events. Then this normalized weight is multiplied with the cross section of the event's class,
so it is considered how often an event according to each class would be observed when measuring real data. To treat all classes equally in the next step the weights for each event of a specific class are normalized
with the sum of all weights from this class, so the sum of weights for each class equals $1$ after normalization. Finally the weights of events that belong to the training data set are only positive, whilst
the weights of events from the other datasets can also be negative. \\

In Fig. \ref{fig:7} the training and validation loss can be seen, Fig. \ref{fig:8} shows the training and validation accuracy.

\Figure{H}{0.8}{fig:7}{Loss plot}{ - Training and validation loss for each epoch.}{content/Plots/performance/loss_plot.png}
\Figure{H}{0.8}{fig:8}{Accuracy plot}{ - Training and validation accuracy for each epoch.}{content/Plots/performance/acc_plot.png}

It is noticeable that the validation loss is a bit higher than the training loss and that the gap between the curves increases. In general it is normal that a neural network shows a better performance
on the data that it is trained with than on unseen data, but since both curves still show a decreasing behaviour, there is no sign of overtraining. 
After epoch $352$ the training is ended by the early stopping, so the weights from epoch $277$ are chosen with this results:

\eqn{
    \mathcal{L} = 0.8888\\
    acc = 0.9162
}

To evaluate the performance of the MLP further the confusion matric and ROC curves are helpful, for all of these plots the validation data is used with the weights described above.
For the confusion matrix a threshold of $0.5$ is set, which means that if the highest entry
of the one-hot encoded prediction is larger than $0.5$ the event is assigned to the corresponding class, otherwise it is not assigned to a class at all. The confusion matrix shows
how many percent of events of each class are actually predicted to that class and how many events are assigned to what other class. The confusion matrix for the MLP can be seen in Fig. \ref{fig:9}.

\Figure{H}{0.8}{fig:9}{Confusion matrix with a threshhold of $0.5$}{}{content/Plots/performance/cm_plot.png}

The plot shows that the non-resonant background could be identified best, followed by the $t \bar{t} H$-background. The model seems to have some difficulty to distinguish
the two signal events from each other, this will be studied later in this chapter. Since $t \bar{t} H$-events shows a more similar kinematic to signal events than the non-resonant background, as it was described in Sec. \ref{sec:bkgproccess}, it is expected
that the non-resonant background can best be differntiated by the MLP. Furthermore especially the ggF-class is often misidentified, especially with the $t \bar{t}H$-background more than the VBF class.
When looking at the feynman graphs of the events (reference!!!) it can be seen that in ggF events the two Higgs bosons are arbitrary distributed in relation to the beam, as it is for the ttH background.
In VBF in contrast, in two production modes the higgs bosons is more likely to go into the beam direction, so these events can better be differtinated from ttH then ggF.
The coliins soper angle which is the higgs bosons direction compared to the beam axis amplifies this theory. \\ 

The ROC-curve shows the True Positive Rate (TPR),

\eqn{
    TPR = \frac{TP}{TP+FN}
}

where the true positives (TP) are the events that were correctly assigned to the considered class and the false negatives are the events that were falsly assigned to one of the not-considered classes,
in dependence of the False Positive Rate (FPR),

\eqn{
    FPR = \frac{FP}{FP+TN}
}

where the false positives are events that were falsly assigned to a specific class and the true negatives are the events that were correctly assigned to a not-considered class.
The threshhold value at which an event is assigned to a specific class is varied between $0$ and $1$ and for each value and class the TPR and FPR are calculated. Then the area under the curve can be estimated,
which should be $1$ in a perfect classification. 

\Figure{H}{0.8}{fig:10}{ROC one vs. all}{}{content/Plots/performance/roc_plot.png}

In Fig. \ref{fig:10} the one vs. all ROC curve is presented,
where all events, that do not belong to the considered class are summarized together. The dashed line represents a random classification, where $50 \%$ of the events would be assigned to the considered
class. It can be observed the same behaviour as in the confusion matrix: The non-resonant background has with a value of $0.99$ the largest area under the curve, for the signal classes these
values are a little bit lower ($0.94$ for ggF and $0.95$ for VBF).

Fig. \ref{fig:11} shows the one vs. one ROC curves for the signal classes vs. each of the background classes, where only the two classes considered are taken into account. 

\Figure{H}{0.8}{fig:11}{ROC one vs. all}{}{content/Plots/performance/combined_roc_curves.png}

Here it can be seen that the two signal classes can better be differentiated from the non-resonant background than from the $t \bar{t}H$-background and again VBF-events are a bit
better differentiated from the $t \bar{t}H$-background than ggF-events. 

% To understand why the two signal events are not well differentiated it makes sense to look how the variables used for training
% are distributed for the signal events, that were assigned to the wrong signal-class.